{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31482c5-1a31-4d91-b52a-ad8050d249a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_gradient_descent(X, y, learning_rate=0.01, iterations=1000):\n",
    "    \"\"\"\n",
    "    Batch Gradient Descent for Linear Regression\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy.ndarray\n",
    "        Training features, shape (n_samples, n_features).\n",
    "    y : numpy.ndarray\n",
    "        Target values, shape (n_samples,).\n",
    "    learning_rate : float, optional\n",
    "        Step size for gradient descent (default is 0.01).\n",
    "    iterations : int, optional\n",
    "        Number of iterations for gradient descent (default is 1000).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    w : numpy.ndarray\n",
    "        Estimated weight vector of shape (n_features,).\n",
    "    b : float\n",
    "        Estimated intercept for the linear regression model.\n",
    "    cost_history : list\n",
    "        List of Mean Squared Error (MSE) values over the gradient descent iterations.\n",
    "    \"\"\"\n",
    "    # Ensure y is a column vector if needed\n",
    "    y = y.reshape(-1)\n",
    "    n = X.shape[0]  # Number of samples\n",
    "    d = X.shape[1]  # Number of features\n",
    "\n",
    "    # Initialize parameters randomly or with zeros\n",
    "    w = np.zeros(d)\n",
    "    b = 0.0\n",
    "\n",
    "    cost_history = []\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        # Compute predictions\n",
    "        y_pred = X @ w + b\n",
    "\n",
    "        # Calculate error\n",
    "        error = y_pred - y\n",
    "\n",
    "        # Compute gradients\n",
    "        dw = (2 / n) * (X.T @ error)\n",
    "        db = (2 / n) * np.sum(error)\n",
    "\n",
    "        # Update parameters\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "\n",
    "        # Compute Mean Squared Error\n",
    "        mse = np.mean(error**2)\n",
    "        cost_history.append(mse)\n",
    "\n",
    "    return w, b, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8987093-c302-4847-a515-349da8ce9bb0",
   "metadata": {},
   "source": [
    "> ## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "616f776e-2e3a-4b3f-9863-210558440577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Weight (w): [2.8174605]\n",
      "Final Bias (b): 2.682389961995458\n",
      "Final MSE: 9.754248675032839\n"
     ]
    }
   ],
   "source": [
    " # Create a synthetic dataset\n",
    "# X with shape (n_samples, 1) for a single feature\n",
    "# y as target values\n",
    "np.random.seed(42)\n",
    "X_demo = np.random.rand(100, 1) * 10  # Features in [0, 10)\n",
    "noise = np.random.normal(0, 3, size=(100,))\n",
    "y_demo = 5 + 2.5 * X_demo[:, 0] + noise  # y = 5 + 2.5*x + some noise\n",
    "\n",
    "# Run Batch Gradient Descent\n",
    "w_final, b_final, cost_hist = batch_gradient_descent(\n",
    "    X_demo, y_demo, learning_rate=0.001, iterations=1000\n",
    ")\n",
    "\n",
    "print(\"Final Weight (w):\", w_final)\n",
    "print(\"Final Bias (b):\", b_final)\n",
    "print(\"Final MSE:\", cost_hist[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
